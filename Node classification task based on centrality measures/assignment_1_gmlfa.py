# -*- coding: utf-8 -*-
"""Assignment_1_gmlfa.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VmYOcb8tA6iiBU-JgAFURaZGC1zwsTRx

# Assignment 1


### This is the supporting Notbook for the given assignment. You will be provided with the instructions and code skeleton of the questions.

1. Please implement the codes on your own cosidering plagarism policy.
2. Write code for corresponding questions in their designated places.
3. Each group have to submit only one notebook (.ipynb) or python (.py) file.

## Group Informtion: To be filled by the candidates.

### Group Number: ____
### Members Roll Numbers: ____
Name: Abhishek Biswas
Roll: 24AI60R40

Name: Akash Badhautiya
Roll: 24AI60R43

Question 1

### A). Write a Python function to generate the specified graph of 50 nodes and plot the graph. (without using any external library) [3 Marks]
"""

import random
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt

# Write a function to generate the graph
def generate_erdos_renyi_graph(n, p=0.1, seed=None):
    """
    Generate an Erdős-Rényi random graph without using external libraries.

    Args:
    n (int): Number of nodes
    p (float): Probability of edge creation between any two nodes (default is 0.1)
    seed (int): Random seed to ensure reproducibility (default is None)

    Returns:
    list: Adjacency matrix representing the generated graph
    """
    # Initialize an n x n adjacency matrix with all zeros
    Graph = [[0 for _ in range(n)] for _ in range(n)]

    # Loop over all pairs of nodes and add edges based on probability p
    for node_u in range(n):
        for node_v in range(node_u + 1, n):  # Only check the upper triangle (since undirected)
            if random.random() < p:  # Add an edge with probability p
                Graph[node_u][node_v] = 1
                Graph[node_v][node_u] = 1  # Ensure the graph is undirected

    return Graph

# Function to convert adjacency mat graph to networkx graph
def convert_to_nx_graph(Graph):
  nx_graph = nx.Graph()
  no_of_nodes = len(Graph)
  for node_u in range(no_of_nodes):
    for node_v in range(node_u + 1, no_of_nodes):
      if Graph[node_u][node_v] == 1:
        nx_graph.add_edge(node_u, node_v)
  return nx_graph


# write a function to plot the graph
def visualize_graph(Graph): # todo
    """
    Visualize the graph using matplotlib.

    Args:
    # todo
    """
    pos = nx.spring_layout(Graph)
    plt.figure(figsize=(10, 10))
    nx.draw(Graph, pos, node_color='lightblue',
            with_labels=True, node_size=500, font_size=10, font_weight='bold')
    plt.title("Erdős-Rényi Random Graph")
    plt.show()

# Generating graph
n = 50
p = 0.1

Graph = generate_erdos_renyi_graph(n, p, 23)
nx_graph = convert_to_nx_graph(Graph)
visualize_graph(nx_graph)

# todo

"""### B). Visualize the Node degree distrbution [refer assignemnt pdf for more detail]. [2 marks]"""

# code here
no_of_nodes = [100, 200, 500, 1000]
probabilities = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]

# Get no. of degrees
def get_no_of_degrees(Graph):
    degrees = []
    for node in range(len(Graph)):
        degree = sum(Graph[node])
        degrees.append(degree)
    return degrees

# Function to visualize degree distribution
def visualize_degree_distribution(n, p, degrees):
    plt.figure(figsize=(8, 6))
    plt.hist(degrees, bins=30, color='lightblue', edgecolor='black')
    plt.title(f'Degree Distribution for G({n}, {p})')
    plt.xlabel('Degree')
    plt.ylabel('Frequency')
    plt.grid(True)
    plt.show()

for nodes in no_of_nodes:
    for prob in probabilities:
        Graph = generate_erdos_renyi_graph(nodes, prob, 23)
        nx_graph = convert_to_nx_graph(Graph)
        degrees = get_no_of_degrees(Graph)
        visualize_degree_distribution(nodes, prob, degrees)

"""#Question 2
### A.) Load the Graph and prepare the labels

- Load the network from torch_geometric.datasets repository
- Prepare one hot labels for each node.

### B.) Feature Engineering and Augmentation

- Compute the following features for each node:
    - Degree centrality
    - Eigenvector centrality
    - Betweenness centrality
    - Local clustering coefficient
    - Closeness centrality
    - PageRank
    - Katz Centrality
- Concatenate these features with existing node features (if available) in the dataset. Prepare a augmented feature for each node.

### C.) Neural Network for Node Classification.

- Preprocess the data and split into training (60%), validation (20%), and test (20%) sets.
- Implement a neural network for multi-class classification (please refer assignemnt pdf for more detail).

"""

!pip install torch_geometric

import networkx as nx
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import torch
import torch.nn as nn
import torch.optim as optim
import torch_geometric
from google.colab import drive

# Load the Graph
def load_graph():
    G = nx.read_edgelist('usa-airports.edgelist', nodetype=int)

    # Convert the graph to edge_index for PyTorch Geometric
    edge_index = torch.tensor(list(G.edges), dtype=torch.long).t().contiguous()

    # Load the labels from 'labels-usa-airports'
    import pandas as pd
    labels = pd.read_csv('labels-usa-airports.txt', sep=' ')
    y = torch.tensor(labels['label'].values, dtype=torch.long)  # Keep labels as LongTensor
    return G, y

# One-hot Label
def prepare_onehot(y):
  # Write code to prepare one-hot label here
  num_of_classes = len(y.unique())
  one_hot_y = torch.nn.functional.one_hot(y, num_classes=num_of_classes)
  return one_hot_y

# Feature Engineering
def compute_features(G):
    deg_cen = nx.degree_centrality(G)
    eig_cen = nx.eigenvector_centrality(G)
    bet_cen = nx.betweenness_centrality(G)
    clust_coeff = nx.clustering(G)
    closeness_cen = nx.closeness_centrality(G)
    page_rank = nx.pagerank(G)

    # Compute eigenvalues
    A = nx.to_numpy_array(G)
    eigenvalues = np.linalg.eigvals(A)
    # Find the largest eigenvalue
    ei_max = np.max(np.abs(eigenvalues))
    katz_cen = nx.katz_centrality(G, 1/ei_max-0.01)

    node_features = np.array([
        [deg_cen[node],
         eig_cen[node],
         bet_cen[node],
         clust_coeff[node],
         closeness_cen[node],
         page_rank[node],
         katz_cen[node]]
        for node in G.nodes
    ])
    return node_features

# Neural Network for Node Classification
class SimpleNN(nn.Module):
    def __init__(self, input_dim):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(input_dim, 64)
        self.fc2 = nn.Linear(64, 48)
        self.fc3 = nn.Linear(48, 32)
        self.fc4 = nn.Linear(32, 4)  # Output layer for 4 classes

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = torch.relu(self.fc3(x))
        x = self.fc4(x)  # Pass through the final linear layer to get logits
        return x

# Part 4: Training
def train_and_evaluate(X_train, y_train, X_val, y_val, X_test, y_test):
    criterion = nn.CrossEntropyLoss()  # Use CrossEntropyLoss for integer labels
    learning_rates = [0.1, 0.01, 0.001, 0.005, 0.0001]
    best_lr = None
    best_val_accuracy = 0
    best_model = None
    input_dim = X_train.shape[1]

    for lr in learning_rates:
        print(f"Tuning for learning rate: {lr}")
        model = SimpleNN(input_dim)
        optimizer_func = optim.Adam(model.parameters(), lr=lr)

        for epoch in range(200):  # Train for 200 epochs
            model.train()
            optimizer_func.zero_grad()
            output = model(X_train)
            loss = criterion(output, y_train)
            loss.backward()
            optimizer_func.step()

            if epoch % 10 == 0:
                model.eval()
                with torch.no_grad():
                    val_output = model(X_val)
                    val_loss = criterion(val_output, y_val)
                    val_pred = torch.argmax(val_output, dim=1)
                    val_accuracy = accuracy_score(y_val, val_pred)
                print(f"Epoch {epoch}, Loss: {loss.item()}, Val Accuracy: {val_accuracy}")

        # Track the best learning rate based on validation accuracy
        if val_accuracy > best_val_accuracy:
            best_val_accuracy = val_accuracy
            best_lr = lr
            best_model = model

        print(f"Best learning rate: {best_lr} with Validation Accuracy: {best_val_accuracy}")

    # Evaluate best model
    best_model.eval()
    with torch.no_grad():
        test_output = best_model(X_test)
        test_pred = torch.argmax(test_output, dim=1)

    return best_model, best_lr, test_pred

#: Report the model's performance metrics.(test accuracy,precision,recall and F1-score)
def preformance_metrics(y_test, test_pred):
    acc = accuracy_score(y_test, test_pred)
    print(f"Test Accuracy: {acc}")

    pre = precision_score(y_test, test_pred, average='macro')
    print(f"Test Precision: {pre}")

    recall = recall_score(y_test, test_pred, average='macro')
    print(f"Test Recall: {recall}")

    f1 = f1_score(y_test, test_pred, average='macro')
    print(f"Test F1-Score: {f1}")


def visualize_classification_result(y_true, y_pred, class_names=None):
    cm = confusion_matrix(y_true, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
    disp.plot(cmap='Blues', values_format='d')
    plt.title('Confusion Matrix')
    plt.show()

# Main execution
if __name__ == '__main__':
    # Generate graph
    Graph, y = load_graph()

    # Compute features
    Graph_features = compute_features(Graph)

    # Split data
    X_train, X_temp, y_train, y_temp = train_test_split(Graph_features, y, test_size=0.4, random_state=31)
    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=31)

    # Standardize features
    scale = StandardScaler()
    X_train_scaled = scale.fit_transform(X_train)
    X_val_scaled = scale.transform(X_val)
    X_test_scaled = scale.transform(X_test)

    # Convert to PyTorch tensors
    X_train_py = torch.FloatTensor(X_train_scaled)
    X_val_py = torch.FloatTensor(X_val_scaled)
    X_test_py = torch.FloatTensor(X_test_scaled)
    y_train_py = torch.LongTensor(y_train.numpy())
    y_val_py = torch.LongTensor(y_val.numpy())
    y_test_py = torch.LongTensor(y_test.numpy())

    # Train and evaluate model
    best_model, best_lr, test_pred = train_and_evaluate(X_train_py, y_train_py, X_val_py, y_val_py, X_test_py, y_test_py)

    # Report metrics
    preformance_metrics(y_test_py, test_pred)

    # Visualize results
    visualize_classification_result(y_test_py.numpy(), test_pred.numpy())

"""## Question: 3

Graph Classification based on Graphlet Degree Vector.

Perform a graph classification task based on graphlet degree vector count  for the NCI109 dataset from the TUDataset collection. The NCI109 dataset is a collection of 4,127 chemical compounds, where each graph represents a compound, and the nodes and edges represent atoms and bonds. Each graph in the dataset is labelled as either "active" or "inactive" against a specific cancer cell line, indicating whether the chemical compound is bioactive or not.
"""

#importing the libraries
import torch
import torch.nn as nn
import torch.optim as optim
from torch_geometric.datasets import TUDataset
from torch_geometric.data import DataLoader
import networkx as nx
import numpy as np
from itertools import combinations
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

## Loading NCI109 dataset

import zipfile
import os

path= '/content/NCI109.zip'

with zipfile.ZipFile(path, 'r') as zip_r:
    zip_r.extractall()

files= os.listdir('/content/NCI109')
print('Files in the directory: ', files)

## Split the dataset
# to load an edge and forming the graph based on edge list
import networkx as nx

def edge_list(path_of_file):
    G = nx.Graph()
    with open(path_of_file, 'r') as file:
        for line in file:
            #spliting the line at comma
            u,v=map(int, line.strip().split(','))
            G.add_edge(u,v) # adding edge

    return G

#Getting the edge list
path_of_file= '/content/NCI109/NCI109_A.txt'
Graph= edge_list(path_of_file)

print(Graph)

import numpy as np

#Loading the indicator of graph
path_indicator= '/content/NCI109/NCI109_graph_indicator.txt'
indicator_graph= np.loadtxt(path_indicator)

indicator_graph.shape

# making a dict. for holding the graph
dict_graph={}

for i, graph_ind in enumerate(indicator_graph):
    if graph_ind not in dict_graph:
        dict_graph[graph_ind]=nx.Graph() #new empty graph
    dict_graph[graph_ind].add_node(i)

print(dict_graph)

#Loading the edge and add to corresponding graph

with open(path_of_file, 'r') as file:
    for line in file:
        #spliting the line at comma , subs 1 for 0 indexing
        u,v=map(lambda x:int(x)-1, line.strip().split(','))
        dict_graph[indicator_graph[u]].add_edge(u,v)
        dict_graph[indicator_graph[v]].add_edge(v,u)

print(dict_graph)

graphs=list(dict_graph.values())

print(dict_graph[1])

print(dict_graph[5])

#checking the graph labels
path_label= '/content/NCI109/NCI109_graph_labels.txt'
graph_labels= np.loadtxt(path_label,dtype=int)

assert len(graphs)==len(graph_labels)
#no of label equal with no of graphs

from itertools import combinations

#count the no of triagles (3 node) in graph
def triangle_count(graph):
  count=0
  for u,v,w in combinations(graph.nodes(),3):
    if graph.has_edge(u,v) and graph.has_edge(v,w ) and graph.has_edge(w,u):
      count+=1
  return count

#count the no of C4(cycle-4) in graph
def C4_count(graph):
  count=0
  for u,v,w,x in combinations(graph.nodes(),4):
    if graph.has_edge(u,v) and graph.has_edge(v,w) and graph.has_edge(w,x) and graph.has_edge(x,u):
      count+=1
  return count

#counting the no of stars centered with min neighbours
def stars_count(graph):
  count=0
  for center in graph.nodes():
    neigh_bors=list(graph.neighbors(center))
    if len(neigh_bors)>=3:
      for neigh_combinations in combinations(neigh_bors,3):
        if all(graph.has_edge(center,neighbor) for neighbor in neigh_combinations):
          count+=1

  return count

# counting the no of hexagon
def hexagon_count(graph):
  count=0
  for u,v,w,x,y,z in combinations(graph.nodes(),6):
    if graph.has_edge(u,v) and graph.has_edge(v,w) and graph.has_edge(w,x) and graph.has_edge(x,y) and graph.has_edge(y,z) and graph.has_edge(z,u) and not graph.has_edge(u,w) and not graph.has_edge(v,x) and not graph.has_edge(w,y) and not graph.has_edge(x,z) and not graph.has_edge(y,u) and not graph.has_edge(z,v):
      count+=1
  return count

#counting the no of pentagon
def pentagon_count(graph):
  count=0
  for u, v,w,x,y in combinations(graph.nodes(),5):
    if graph.has_edge(u,v) and graph.has_edge(v,w) and graph.has_edge(w,x) and graph.has_edge(x,y) and graph.has_edge(y,u) and not graph.has_edge(u,w) and not graph.has_edge(v,x) and not graph.has_edge(w,y) and not graph.has_edge(x,u) and not graph.has_edge(y,v):
      count+=1
  return count

# complete graph k4
def k4_count(graph):
  count=0
  for u,v,w,x in combinations(graph.nodes(),4):
    if graph.has_edge(u,v) and graph.has_edge(u,w)  and graph.has_edge(v,w) and graph.has_edge(u,x) and graph.has_edge(w,x) and graph.has_edge(v,x):
      count+=1
  return count

#complete graph k5
def k5_count(graph):
  count=0
  for u,v,w,x,y in combinations(graph.nodes(),5):
    if graph.has_edge(u,v) and graph.has_edge(u,w)  and graph.has_edge(u,x) and graph.has_edge(v,w) and graph.has_edge(v,x) and graph.has_edge(w,x) :
      count+=1
  return count

# complete graph k6
def k6_count(graph):
  count=0
  for u,v,w,x,y,z in combinations(graph.nodes(),6):
    if graph.has_edge(u,v) and graph.has_edge(u,w) and graph.has_edge(u,x) and graph.has_edge(v,w) and graph.has_edge(v,x) and graph.has_edge(w,x) and graph.has(u,y) and graph.has_edge(v,y) and graph.has_edge(w,y) and graph.has_edge(x,y) and graph.has(u,z) and graph.has_edge(v,z) and graph.has_edge(w,z) and graph.has_edge(x,z) and graph.has(y,z):
      count+=1
  return count

# count linear chain of 3 node
def chain_count(graph):
  count=0
  for u,v,w in combinations(graph.nodes(),3):
    if graph.has_edge(u,v) and graph.has_edge(v,w) and not graph.has_edge(u,w):
      count+=1
  return count

#list of vector containing the count of different type
def extract_graphlet_deg_vector(G):
  deg_vector=[
      triangle_count(G),
      hexagon_count(G),
      pentagon_count(G),
      k4_count(G),
      k5_count(G),
      k6_count(G),
      C4_count(G),
      stars_count(G),
      chain_count(G),]


  return deg_vector

feature_vect=[extract_graphlet_deg_vector(graph) for graph in graphs]

for j, vect in enumerate(feature_vect):
  print(f"Graph {j}: {vect}")

"""# Part 2"""

from sklearn.preprocessing import StandardScaler
import numpy as np

#converting the feature vect
x=np.array(feature_vect)
y=np.array(graph_labels)

#scaling the feature vect
scaler=StandardScaler()
x_scaled=scaler.fit_transform(x)

from sklearn.model_selection import train_test_split

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.2, random_state=42)

#importing the libraries
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, classification_report

#----------Neural network Model----------
class Simple_NN(nn.Module):
  def __init__(self, input_size, hidden_size, output_size):
    super(Simple_NN, self).__init__()
    self.fc1=nn.Linear(input_size, hidden_size)
    self.fc2=nn.Linear(hidden_size, hidden_size)
    self.fc3=nn.Linear(hidden_size, output_size)
    self.relu=nn.ReLU()
    self.softmax=nn.nn.Softmax(dim=1)


  def for_ward(self, x):
    x=self.relu(self.fc1(x))
    x=self.relu(self.fc2(x))
    x=self.fc3(x)
    x=self.softmax(x)
    return x

# translating data to pytorch tensors
X_train_tensor=torch.tensor(X_train, dtype=torch.float32)
X_test_tensor=torch.tensor(X_test, dtype=torch.float32)
y_train_tensor=torch.tensor(y_train, dtype=torch.long)
y_test_tensor=torch.tensor(y_test, dtype=torch.long)

#initiating the  model ,loss fn ,optimizer
input_size=X_train_tensor.shape[1]
hidden_size=64
output_size=2 #BInary class.
model=Simple_NN(input_size, hidden_size, output_size)
criterion=nn.CrossEntropyLoss()
optimizer=optim.Adam(model.parameters(), lr=0.001)

#Train the model
num_iter=100
for iter in range(num_iter):
  model.train()
  optimizer.zero_grad()

  outputs=model(X_train_tensor)
  loss=criterion(outputs, y_train_tensor)
  loss.backward()
  optimizer.step()
  if (iter+1)%10==0:
    print(f'Epoch [{iter+1}/{num_iter}], Loss: {loss.item():.4f}')

#Evaluating  Model
model.eval()
with torch.no_grad():
  y_pre_pro=model(X_test_tensor)
  y_pre=torch.argmax(y_pre_pro, dim=1).numpy()
  y_test_nP=y_test_tensor.numpy()

#Printing the Performace metrices
print('Classification report: \n',classification_report(y_test_nP, y_pre))

#Printing the Accuracry
print('Accuracy: ',accuracy_score(y_test_nP, y_pre))

#importing the library
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot

confusion__matrix=confusion_matrix(y_test_nP, y_pre)
plt.figure(figsize=(8,6))

sns.heatmap(confusion__matrix, annot=True, fmt='d', cmap='Blues', yticklabels=['INACTIVE','ACTIVE'], xticklabels=['INACTIVE','ACTIVE'], cbar=False)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

!pip install shap

import shap
def prec_pro(x):
  with torch.no_grad():

    if not isinstance(x, torch.Tensor):
      x=torch.tensor(x, dtype=torch.float32)
    if x.device != next(model.parameters()).device:
      x=x.to(next(model.parameters()).device)
    output=model(x).numpy()
    return output

from math import exp
# Create a SHAP KernelExplainer
explainer = shap.KernelExplainer(prec_pro, X_train_tensor.numpy())

sample_size=100
shap_values = explainer.shap_values(X_test_tensor.numpy()[:sample_size], nsamples=100)

import pandas as pd

shap_df =pd.DataFrame(np.mean(np.abs(shap_values[0]), axis=0), columns=['importance'])

#ploting
plt.figure(figsize=(10,6))
plt.bar(range(len(shap_df)), shap_df['importance'])
plt.xlabel('Features')
plt.ylabel('Importance')
plt.title('Feature Importance')
plt.show()